[
  {
    "objectID": "example1.html",
    "href": "example1.html",
    "title": "Example 1",
    "section": "",
    "text": "This example was created by converting the code provided by Alina into a quarto QMD file.\nHere is the image that will be processed:\nimage_filename = \"../sample-code/ad2_f29.JPG\"\nimage_filename\n\n'../sample-code/ad2_f29.JPG'"
  },
  {
    "objectID": "example1.html#load-libraries-used",
    "href": "example1.html#load-libraries-used",
    "title": "Example 1",
    "section": "Load libraries used",
    "text": "Load libraries used\n\n#Segment Anything Model\n#Alina Zaidi\n\n#imports\n\nimport sys\nimport cv2\nimport time\nimport torch\nimport torchvision\nimport platform\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#sys.path.append(\"..\")\nfrom pathlib import Path\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"Torchvision version:\", torchvision.__version__)\nprint(\"CUDA is available:\", torch.cuda.is_available())\n\nPyTorch version: 2.8.0+cu128\nTorchvision version: 0.23.0+cu128\nCUDA is available: True"
  },
  {
    "objectID": "example1.html#load-the-image-and-write-out-its-pixel-size.",
    "href": "example1.html#load-the-image-and-write-out-its-pixel-size.",
    "title": "Example 1",
    "section": "Load the image and write out it’s pixel size.",
    "text": "Load the image and write out it’s pixel size.\n\n#Upload image and modify for model\n\nimage_filename = \"../sample-code/ad2_f29.JPG\"\nimage_filename = Path( image_filename )\nif not image_filename.is_file():\n    print(f\"{image_filename} does not exist\")\n    sys.exit(1)\n\noriginal_image = cv2.imread( image_filename )  #change to match file name\nprint(original_image.shape)\n\n(1944, 2592, 3)\n\n\nSwap RGB channels in preparation for the model.\n\nimage = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\nprint(image.shape)\n\n(1944, 2592, 3)"
  },
  {
    "objectID": "example1.html#plot-the-image",
    "href": "example1.html#plot-the-image",
    "title": "Example 1",
    "section": "Plot the image",
    "text": "Plot the image\n\nplt.figure(figsize=(5,5), dpi = 100)\nplt.imshow(image)\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "example1.html#show-side-by-side-images",
    "href": "example1.html#show-side-by-side-images",
    "title": "Example 1",
    "section": "show side-by-side images",
    "text": "show side-by-side images\n\n# Convert\n# Show before and after\nfig, axes = plt.subplots(1, 2, figsize=(10,5))\n\naxes[0].imshow(original_image)\naxes[0].set_title(\"Original\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(image)\naxes[1].set_title(\"Adjusted\")\naxes[1].axis(\"off\")\n\nplt.show()"
  },
  {
    "objectID": "example1.html#perform-an-analysis",
    "href": "example1.html#perform-an-analysis",
    "title": "Example 1",
    "section": "Perform an analysis",
    "text": "Perform an analysis\n\ndetect and set the processor to use\n\nif torch.cuda.is_available():\n    cc_major, cc_minor = torch.cuda.get_device_capability(0)\n    if cc_major &lt; 7:  # unsupported by current PyTorch build\n        print(f\"GPU {torch.cuda.get_device_name(0)} (compute capability {cc_major}.{cc_minor})\\n\"\n              f\"is not supported by this PyTorch build. Falling back to CPU.\")\n        device = torch.device(\"cpu\")\n        print(\"Using CPU\")\n        print(\"CPU info:\", platform.processor())\n        print(\"Machine type:\", platform.machine())\n        print(\"Platform:\", platform.platform())\n    else:\n        device = torch.device(\"cuda:0\")\n        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU\")\n    print(\"CPU info:\", platform.processor())\n    print(\"Machine type:\", platform.machine())\n    print(\"Platform:\", platform.platform())\n\nUsing GPU: NVIDIA RTX A3000 12GB Laptop GPU\n\n\n\n\nload the model\nThis model is obtained from the Meta.\n\n#what sam model to use (see their github for diff options, but b is the smallest one = less crashing)\nsam_checkpoint = \"sam_vit_b_01ec64.pth\"\nmodel_type = \"vit_b\"\n\n#added print statements to check due to previous crashing issues\ntry: \n    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n    print(\"YUP all loaded\")\nexcept Exception as e:\n    print(\"NOPE not loaded\")\n\nYUP all loaded\n\n\n\n\nSet up to run the SAM model\n\nsam.to(device=device)\n\n#Below are parameters that seem to be good for chaing min size allowed, how detailed the model looks for masks, etc \n\n\n#There are several tunable parameters in automatic mask generation that control \n# how densely points are sampled and what the thresholds are for removing low \n# quality or duplicate masks. Additionally, generation can be automatically \n# run on crops of the image to get improved performance on smaller objects, \n# and post-processing can remove stray pixels and holes. \n# Here is an example configuration that samples more masks:\n#https://github.com/facebookresearch/segment-anything/blob/9e1eb9fdbc4bca4cd0d948b8ae7fe505d9f4ebc7/segment_anything/automatic_mask_generator.py#L35    \n\n#Rerun the following with a few settings, ex. 0.86 & 0.9 for iou_thresh\n# and 0.92 and 0.96 for score_thresh\n\nmask_generator_ = SamAutomaticMaskGenerator(\n     model=sam,\n    points_per_side=32,\n    pred_iou_thresh=0.9,\n    stability_score_thresh=0.96,\n    crop_n_layers=1,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=100,    # Requires open-cv to run post-processing\n)\n\nApply the defined mask generator to process the image.\nThis a very expensive (time consuming) process.\n\nstart = time.perf_counter()\n\nmasks = mask_generator_.generate(image)\n\nend = time.perf_counter()\nelapsed = end - start\n\nprint(\"The number of masks:\", len(masks))\nprint(f\"Mask generation took {elapsed:.2f} seconds\")\n\nThe number of masks: 18\nMask generation took 18.77 seconds\n\n\n\n\nPresent image with cells identified\nUsing the masks created above, plot the segments/cells.\n\ndef show_anns(anns):\n    # Function to loop over segments identified and display them on plot\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n    polygons = []\n    color = []\n    for ann in sorted_anns:\n        m = ann['segmentation']\n        img = np.ones((m.shape[0], m.shape[1], 3))\n        color_mask = np.random.random((1, 3)).tolist()[0]\n        for i in range(3):\n            img[:,:,i] = color_mask[i]\n        ax.imshow(np.dstack((img, m*0.35)))\n\n#output the image with colors for the masks\n\nplt.figure(figsize=(5,5))\nplt.imshow(image)\nshow_anns(masks)\nplt.axis('off')\nplt.show() \n\n\n\n\n\n\n\n\n\n\nCompute area\n\n#analysis\ntotal_areas = [] #initialize the array\nfor mask in masks:\n    #check if the mask is a bubble\n    if mask[\"area\"] &gt; 350:\n        # add non-bubble masks to the array\n        total_areas.append(mask[\"area\"])\n\n#remove the background element\ndel total_areas[0]\n\n#convert to a numpy array\ntotal_areas = np.array(total_areas)\n#sort the array from least to greatest (just easier to look at)\ntotal_areas.sort()\n\nfor area in total_areas:\n    print(area)\n\n511\n588\n1022\n2224\n3194\n3438\n3772\n4221\n4988689\n\n\n\n\nCompute\n\n#find images average area in pixels\nmean_area = np.mean(total_areas)\n\nprint(f\"The mean area in pixels is {mean_area:.2f}\")\n\nThe mean area in pixels is 556406.56\n\n\n\n\nPerform cell classificition\nWe’ll identify cells based on cell size (area)\n\n#Attempt to find overall cell type based on mean of cell area \nif mean_area &gt;= 10000:\n    print(\"buccal cells\")\nelif mean_area &lt;2500:\n    print(\"touch cells\")\nelif mean_area &gt;=2500 and mean_area &lt; 10000:\n    print(\"saliva cells\")\n\n# how many of each type\nbuccalCount = 0\ntouchCount = 0\nsalivaCount = 0\n\nfor area in total_areas:\n    if area &gt;= 10000:\n        buccalCount +=1\n    elif area &lt;2500:\n        touchCount +=1\n    elif area &gt;=2500 and area &lt; 10000:\n        salivaCount +=1\n\nprint(f\"\"\"\nThere are:\n{buccalCount} buccal cells,\n{touchCount} touch cells, and\n{salivaCount} saliva cells\n\"\"\")\n\nbuccal cells\n\nThere are:\n1 buccal cells,\n4 touch cells, and\n4 saliva cells"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "First draft of a working website documenting our work on this project."
  },
  {
    "objectID": "index.html#intro-presentation",
    "href": "index.html#intro-presentation",
    "title": "Welcome",
    "section": "Intro presentation",
    "text": "Intro presentation\nBelow is the intro presentation shared by Alina. Here a direct link to the google slides source."
  }
]