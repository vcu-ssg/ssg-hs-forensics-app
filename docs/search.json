[
  {
    "objectID": "example1.html",
    "href": "example1.html",
    "title": "Example 1",
    "section": "",
    "text": "This example was created by converting the code provided by Alina into a quarto QMD file.\nHere is the image that will be processed:\nimage_filename = \"../sample-code/ad2_f29.JPG\"\nimage_filename\n\n'../sample-code/ad2_f29.JPG'"
  },
  {
    "objectID": "example1.html#load-libraries-used",
    "href": "example1.html#load-libraries-used",
    "title": "Example 1",
    "section": "Load libraries used",
    "text": "Load libraries used\n\n#Segment Anything Model\n#Alina Zaidi\n\n#imports\n\nimport sys\nimport cv2\nimport time\nimport torch\nimport torchvision\nimport platform\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#sys.path.append(\"..\")\nfrom pathlib import Path\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"Torchvision version:\", torchvision.__version__)\nprint(\"CUDA is available:\", torch.cuda.is_available())\n\nPyTorch version: 2.8.0+cu128\nTorchvision version: 0.23.0+cu128\nCUDA is available: True"
  },
  {
    "objectID": "example1.html#load-the-image-and-write-out-its-pixel-size.",
    "href": "example1.html#load-the-image-and-write-out-its-pixel-size.",
    "title": "Example 1",
    "section": "Load the image and write out it’s pixel size.",
    "text": "Load the image and write out it’s pixel size.\n\n#Upload image and modify for model\n\nimage_filename = \"../sample-code/ad2_f29.JPG\"\nimage_filename = Path( image_filename )\nif not image_filename.is_file():\n    print(f\"{image_filename} does not exist\")\n    sys.exit(1)\n\noriginal_image = cv2.imread( image_filename )  #change to match file name\nprint(original_image.shape)\n\n(1944, 2592, 3)\n\n\nSwap RGB channels in preparation for the model.\n\nimage = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\nprint(image.shape)\n\n(1944, 2592, 3)"
  },
  {
    "objectID": "example1.html#plot-the-image",
    "href": "example1.html#plot-the-image",
    "title": "Example 1",
    "section": "Plot the image",
    "text": "Plot the image\n\nplt.figure(figsize=(5,5), dpi = 100)\nplt.imshow(image)\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "example1.html#show-side-by-side-images",
    "href": "example1.html#show-side-by-side-images",
    "title": "Example 1",
    "section": "show side-by-side images",
    "text": "show side-by-side images\n\n# Convert\n# Show before and after\nfig, axes = plt.subplots(1, 2, figsize=(10,5))\n\naxes[0].imshow(original_image)\naxes[0].set_title(\"Original\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(image)\naxes[1].set_title(\"Adjusted\")\naxes[1].axis(\"off\")\n\nplt.show()"
  },
  {
    "objectID": "example1.html#perform-an-analysis",
    "href": "example1.html#perform-an-analysis",
    "title": "Example 1",
    "section": "Perform an analysis",
    "text": "Perform an analysis\n\ndetect and set the processor to use\n\nif torch.cuda.is_available():\n    cc_major, cc_minor = torch.cuda.get_device_capability(0)\n    if cc_major &lt; 7:  # unsupported by current PyTorch build\n        print(f\"GPU {torch.cuda.get_device_name(0)} (compute capability {cc_major}.{cc_minor})\\n\"\n              f\"is not supported by this PyTorch build. Falling back to CPU.\")\n        device = torch.device(\"cpu\")\n        print(\"Using CPU\")\n        print(\"CPU info:\", platform.processor())\n        print(\"Machine type:\", platform.machine())\n        print(\"Platform:\", platform.platform())\n    else:\n        device = torch.device(\"cuda:0\")\n        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU\")\n    print(\"CPU info:\", platform.processor())\n    print(\"Machine type:\", platform.machine())\n    print(\"Platform:\", platform.platform())\n\nUsing GPU: NVIDIA RTX A3000 12GB Laptop GPU\n\n\n\n\nload the model\nThis model is obtained from the Meta.\n\n#what sam model to use (see their github for diff options, but b is the smallest one = less crashing)\nsam_checkpoint = \"sam_vit_b_01ec64.pth\"\nmodel_type = \"vit_b\"\n\n#added print statements to check due to previous crashing issues\ntry: \n    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n    print(\"YUP all loaded\")\nexcept Exception as e:\n    print(\"NOPE not loaded\")\n\nYUP all loaded\n\n\n\n\nSet up to run the SAM model\n\nsam.to(device=device)\n\n#Below are parameters that seem to be good for chaing min size allowed, how detailed the model looks for masks, etc \n\n\n#There are several tunable parameters in automatic mask generation that control \n# how densely points are sampled and what the thresholds are for removing low \n# quality or duplicate masks. Additionally, generation can be automatically \n# run on crops of the image to get improved performance on smaller objects, \n# and post-processing can remove stray pixels and holes. \n# Here is an example configuration that samples more masks:\n#https://github.com/facebookresearch/segment-anything/blob/9e1eb9fdbc4bca4cd0d948b8ae7fe505d9f4ebc7/segment_anything/automatic_mask_generator.py#L35    \n\n#Rerun the following with a few settings, ex. 0.86 & 0.9 for iou_thresh\n# and 0.92 and 0.96 for score_thresh\n\nmask_generator_ = SamAutomaticMaskGenerator(\n     model=sam,\n    points_per_side=32,\n    pred_iou_thresh=0.9,\n    stability_score_thresh=0.96,\n    crop_n_layers=1,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=100,    # Requires open-cv to run post-processing\n)\n\nApply the defined mask generator to process the image.\nThis a very expensive (time consuming) process.\n\nstart = time.perf_counter()\n\nmasks = mask_generator_.generate(image)\n\nend = time.perf_counter()\nelapsed = end - start\n\nprint(\"The number of masks:\", len(masks))\nprint(f\"Mask generation took {elapsed:.2f} seconds\")\n\nThe number of masks: 18\nMask generation took 18.77 seconds\n\n\n\n\nPresent image with cells identified\nUsing the masks created above, plot the segments/cells.\n\ndef show_anns(anns):\n    # Function to loop over segments identified and display them on plot\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n    polygons = []\n    color = []\n    for ann in sorted_anns:\n        m = ann['segmentation']\n        img = np.ones((m.shape[0], m.shape[1], 3))\n        color_mask = np.random.random((1, 3)).tolist()[0]\n        for i in range(3):\n            img[:,:,i] = color_mask[i]\n        ax.imshow(np.dstack((img, m*0.35)))\n\n#output the image with colors for the masks\n\nplt.figure(figsize=(5,5))\nplt.imshow(image)\nshow_anns(masks)\nplt.axis('off')\nplt.show() \n\n\n\n\n\n\n\n\n\n\nCompute area\n\n#analysis\ntotal_areas = [] #initialize the array\nfor mask in masks:\n    #check if the mask is a bubble\n    if mask[\"area\"] &gt; 350:\n        # add non-bubble masks to the array\n        total_areas.append(mask[\"area\"])\n\n#remove the background element\ndel total_areas[0]\n\n#convert to a numpy array\ntotal_areas = np.array(total_areas)\n#sort the array from least to greatest (just easier to look at)\ntotal_areas.sort()\n\nfor area in total_areas:\n    print(area)\n\n511\n588\n1022\n2224\n3194\n3438\n3772\n4221\n4988689\n\n\n\n\nCompute\n\n#find images average area in pixels\nmean_area = np.mean(total_areas)\n\nprint(f\"The mean area in pixels is {mean_area:.2f}\")\n\nThe mean area in pixels is 556406.56\n\n\n\n\nPerform cell classificition\nWe’ll identify cells based on cell size (area)\n\n#Attempt to find overall cell type based on mean of cell area \nif mean_area &gt;= 10000:\n    print(\"buccal cells\")\nelif mean_area &lt;2500:\n    print(\"touch cells\")\nelif mean_area &gt;=2500 and mean_area &lt; 10000:\n    print(\"saliva cells\")\n\n# how many of each type\nbuccalCount = 0\ntouchCount = 0\nsalivaCount = 0\n\nfor area in total_areas:\n    if area &gt;= 10000:\n        buccalCount +=1\n    elif area &lt;2500:\n        touchCount +=1\n    elif area &gt;=2500 and area &lt; 10000:\n        salivaCount +=1\n\nprint(f\"\"\"\nThere are:\n{buccalCount} buccal cells,\n{touchCount} touch cells, and\n{salivaCount} saliva cells\n\"\"\")\n\nbuccal cells\n\nThere are:\n1 buccal cells,\n4 touch cells, and\n4 saliva cells"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "First draft of a working website documenting our work on this project."
  },
  {
    "objectID": "index.html#intro-presentation",
    "href": "index.html#intro-presentation",
    "title": "Welcome",
    "section": "Intro presentation",
    "text": "Intro presentation\nBelow is the intro presentation shared by Alina. Here a direct link to the google slides source."
  },
  {
    "objectID": "image-gallery-guide.html",
    "href": "image-gallery-guide.html",
    "title": "Image Gallery and Upload Guide",
    "section": "",
    "text": "This guide explains how to use the forensic cell image gallery and upload functionality in the SSG High School Forensics Application. The system allows users to view sample images, upload their own cell images, and process them using AI-powered analysis tools."
  },
  {
    "objectID": "image-gallery-guide.html#overview",
    "href": "image-gallery-guide.html#overview",
    "title": "Image Gallery and Upload Guide",
    "section": "",
    "text": "This guide explains how to use the forensic cell image gallery and upload functionality in the SSG High School Forensics Application. The system allows users to view sample images, upload their own cell images, and process them using AI-powered analysis tools."
  },
  {
    "objectID": "image-gallery-guide.html#image-gallery",
    "href": "image-gallery-guide.html#image-gallery",
    "title": "Image Gallery and Upload Guide",
    "section": "Image Gallery",
    "text": "Image Gallery\nThe image gallery is organized into three main sections, each serving a specific purpose in the forensic analysis workflow.\n\nSample Images\nThe Sample Images section contains pre-loaded reference images of different cell types:\n\nBuccal Cells: Epithelial cells collected from the inside of the cheek, commonly used in forensic DNA analysis\nEpidermal Cells: Skin cells that can be found in fingerprints and touch DNA samples\nSaliva Cells: A mixture of epithelial cells found in biological evidence\n\nThese images are provided for educational purposes and can be used to:\n\nLearn to identify different cell types\nPractice with the image processing tools\nCompare against uploaded images\n\n\nUsing Sample Images\n\nNavigate to the gallery page\nBrowse images organized by cell type\nHover over any image to reveal action buttons:\n\nView: Opens the full-size image in a new tab\nProcess: Sends the image to the AI analysis pipeline\n\n\n\n\n\nUploaded Images\nThis section displays all images that have been uploaded by users through the upload tool.\n\nHow to Upload Images\n\nNavigate to the Upload page\nClick the upload area or drag and drop an image file\nSupported formats: JPG, JPEG, PNG\nOnce uploaded, the image will appear in the Uploaded Images section of the gallery\n\n\n\nProcessing Uploaded Images\nAfter uploading an image:\n\nGo to the gallery page\nFind your image in the Uploaded Images section\nHover over the image to reveal action buttons\nClick Process to analyze the image\nWait for the processing to complete (indicated by status messages)\nView the processed result in the Processed Images section\n\n\n\n\nProcessed Images\nThis section contains all images that have been analyzed by the AI segmentation and identification tools.\nProcessed images include:\n\nSegmented cell boundaries\nCell type identification\nAnalysis metadata\n\n\nViewing Processed Results\n\nClick on any processed image to view it full-size\nProcessed images are named with a processed_ prefix\nResults are stored persistently and can be reviewed at any time"
  },
  {
    "objectID": "image-gallery-guide.html#technical-implementation",
    "href": "image-gallery-guide.html#technical-implementation",
    "title": "Image Gallery and Upload Guide",
    "section": "Technical Implementation",
    "text": "Technical Implementation\n\n\n\n\n\n\nNote\n\n\n\nThis section contains technical details about the system architecture and implementation. Feel free to skip this if you’re primarily interested in using the application.\n\n\n\nArchitecture\nThe gallery system uses a dynamic, API-driven architecture:\nFrontend (Quarto/HTML/JS) → NGINX → FastAPI Backend → File System\n\n\nAPI Endpoints\nThe backend provides three main endpoints for the gallery:\n\nGET /api/gallery/sample-images: Returns organized list of sample images by cell type\nGET /api/gallery/uploaded-images: Returns list of user-uploaded images\nGET /api/gallery/processed-images: Returns list of processed images\n\n\n\nImage Storage\nImages are organized in the following directory structure:\nimages/\n├── sample-gallery-images/\n│   ├── buccal_cells/\n│   ├── epidermal_cells/\n│   └── saliva_cells/\n├── uploaded-images/\n└── processed-images/\n\n\nProcessing Workflow\n\nUser selects an image to process\nFrontend fetches the image as a blob\nImage is sent to /api/process-img endpoint via POST request\nBackend processes the image using the SAM2 model (or current processing logic)\nProcessed image is saved to processed-images/ directory\nFrontend refreshes the Processed Images gallery\nUser is notified of successful processing"
  },
  {
    "objectID": "image-gallery-guide.html#best-practices",
    "href": "image-gallery-guide.html#best-practices",
    "title": "Image Gallery and Upload Guide",
    "section": "Best Practices",
    "text": "Best Practices\n\nFor Students\n\nStart by exploring sample images to learn different cell types\nUpload clear, well-focused microscope images for best results\nProcess images one at a time to avoid overwhelming the system\nCompare processed results against sample images to verify accuracy\n\n\n\nFor Educators\n\nUse sample images for demonstrations and teaching\nGuide students to upload their own collected samples\nReview processed images with students to discuss cell identification\nLeverage the gallery for comparative analysis exercises"
  },
  {
    "objectID": "image-gallery-guide.html#troubleshooting",
    "href": "image-gallery-guide.html#troubleshooting",
    "title": "Image Gallery and Upload Guide",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nImages Not Loading\n\nCheck that Docker containers are running\nVerify volume mounts in docker-compose.yml\nCheck browser console for API errors\n\n\n\nUpload Fails\n\nEnsure file is a supported format (JPG, PNG)\nCheck file size is reasonable\nVerify backend is accessible at /api\n\n\n\nProcessing Fails\n\nCheck that the image file exists and is accessible\nVerify backend processing pipeline is configured\nReview backend logs for error messages"
  },
  {
    "objectID": "image-gallery-guide.html#future-enhancements",
    "href": "image-gallery-guide.html#future-enhancements",
    "title": "Image Gallery and Upload Guide",
    "section": "Future Enhancements",
    "text": "Future Enhancements\nPotential improvements to the gallery system:\n\nImage metadata display (size, dimensions, upload date)\nFiltering and search capabilities\nBatch processing support\nExport functionality for processed images\nComparison view for side-by-side analysis\nUser annotations and notes"
  },
  {
    "objectID": "development.html",
    "href": "development.html",
    "title": "Developer Documentation",
    "section": "",
    "text": "NOTE 1 - TODO: We likely need to adjust the images folder. The sample gallery images should also contain the processed gallery images so (1) users don’t need to wait for sample images to be processed and (2) the processed gallery images are kept separate from the user processed images.\nNOTE 2: What else needs to be added to this page of documentation? I don’t want to get too into the weeds, especially since a lot of information for developers should be found in comments near the code in question."
  },
  {
    "objectID": "development.html#who-this-page-is-for",
    "href": "development.html#who-this-page-is-for",
    "title": "Developer Documentation",
    "section": "Who This Page Is For",
    "text": "Who This Page Is For\nThis section of the documentation is intended for developers or those who are hoping to look under the hood of this project. If you know little about coding or are trying to figure out how to add your own lessons to the application, then this section of documentation is NOT for you. If you are looking for documentation about how to make your own lesson, please see ADD DOCUMENTATION NAME."
  },
  {
    "objectID": "development.html#location-of-files",
    "href": "development.html#location-of-files",
    "title": "Developer Documentation",
    "section": "Location of Files",
    "text": "Location of Files\nThese file locations will be relative to the root directory (ssg-hs-forensics-app/).\nforensics-ui/ - contains the files used to generate the graphics (the actual website itself). Within this directory: * _site/ - contains the rendered pages of the website. * .quarto - contains the intermediate files generated by Quarto before the files in ’_site/’ are created. (DOUBLE CHECK THAT THIS IS CORRECT) * cell-gallery/ - contains the Quarto/QMD and CSS files needed to generate the interactive cell gallery; whenever the ‘process’ button is pressed, it sends the specific image to the backend for processing and retrieves the processed image. * lesson-1/ - contains the QMD and CSS files needed to generate the lesson pages of the first lesson. * upload/ - contains the QMD and CSS files responsible for upload functionality; sends uploaded images to the backend for processing and retrieves the results. * _quarto.yml - file responsible for coordinating the other files for the front-end, and provides the website bar necessary to navigate the different pages of the website.\nforensics-backend/ - contains the files that manage the whole system and process uploaded images. Within this directory: * app/ - contains the following files: - Dockerfile - manages setting up the Docker compartments and running the server. - main.py - contains the server functions, which allow images to be processed and retrieved. - requirements.txt - contains the names of Python libraries required for this project; ‘Dockerfile’ uses this file to know what libraries to install when setting up the Docker compartments and webserver. - run_sam2.py - responsible for processing uploaded images; used by ‘main.py’. * nginx/ - contains the following file: - default.conf - configurations for the Docker compartments; manages pathways between systems, including connecting containers to the outside world and rerouting calls to their intended compartments. * docker-compose.yml - responsible for Docker container creation, and leaves ‘windows’ open so files outside certain compartments can be used.\nimages/ - contains the images used in the website. Within this directory: * processed-images/ - contains the processed images (images that have been run through the SAM model). * sample-gallery-images/ - contains the sample images that are pre-loaded into the gallery. * uploaded-images/ - contains the images users have uploaded.\nreports/ - contains the QMD files that render into the documentation for this project. Within this directory: (TODO: ADD TO) * development.qmd - the file that renders this very file; documentation indended for developers or those who are interested in the behind-the-scenes information."
  },
  {
    "objectID": "development.html#overview-of-coding-languages-used",
    "href": "development.html#overview-of-coding-languages-used",
    "title": "Developer Documentation",
    "section": "Overview of Coding Languages Used",
    "text": "Overview of Coding Languages Used\nThis project primarily uses Python, JavaScript, CSS, and HTML (through Quarto). The actual processing of files relies on Python code. JavaScript is used within the QMD (Quarto) files in order to add interactivity to the generated HTML pages, including APIs that allow images to be sent to the backend server for processing and to request images from the server. Quarto is used to generate the HTML files that make up the website."
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "Welcome",
    "section": "Documentation",
    "text": "Documentation\n\nImage Gallery and Upload Guide - Learn how to upload images and use the gallery features\nDeveloper Documentation - Technical documentation for developers working on the project"
  }
]