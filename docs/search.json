[
  {
    "objectID": "example1.html",
    "href": "example1.html",
    "title": "Example 1",
    "section": "",
    "text": "This example was created by converting the code provided by Alina into a quarto QMD file.\nHere is the image that will be processed:\nimage_filename = \"../sample-code/ad2_f29.JPG\"\nimage_filename\n\n'../sample-code/ad2_f29.JPG'"
  },
  {
    "objectID": "example1.html#load-libraries-used",
    "href": "example1.html#load-libraries-used",
    "title": "Example 1",
    "section": "Load libraries used",
    "text": "Load libraries used\n\n#Segment Anything Model\n#Alina Zaidi\n\n#imports\n\nimport sys\nimport cv2\nimport time\nimport torch\nimport torchvision\nimport platform\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#sys.path.append(\"..\")\nfrom pathlib import Path\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"Torchvision version:\", torchvision.__version__)\nprint(\"CUDA is available:\", torch.cuda.is_available())\n\nPyTorch version: 2.8.0+cpu\nTorchvision version: 0.23.0+cpu\nCUDA is available: False"
  },
  {
    "objectID": "example1.html#load-the-image-and-write-out-its-pixel-size.",
    "href": "example1.html#load-the-image-and-write-out-its-pixel-size.",
    "title": "Example 1",
    "section": "Load the image and write out it’s pixel size.",
    "text": "Load the image and write out it’s pixel size.\n\n#Upload image and modify for model\n\nimage_filename = \"../sample-code/ad2_f29.JPG\"\nimage_filename = Path( image_filename )\nif not image_filename.is_file():\n    print(f\"{image_filename} does not exist\")\n    sys.exit(1)\n\noriginal_image = cv2.imread( image_filename )  #change to match file name\nprint(original_image.shape)\n\n(1944, 2592, 3)\n\n\nSwap RGB channels in preparation for the model.\n\nimage = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\nprint(image.shape)\n\n(1944, 2592, 3)"
  },
  {
    "objectID": "example1.html#plot-the-image",
    "href": "example1.html#plot-the-image",
    "title": "Example 1",
    "section": "Plot the image",
    "text": "Plot the image\n\nplt.figure(figsize=(5,5), dpi = 100)\nplt.imshow(image)\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "example1.html#show-side-by-side-images",
    "href": "example1.html#show-side-by-side-images",
    "title": "Example 1",
    "section": "show side-by-side images",
    "text": "show side-by-side images\n\n# Convert\n# Show before and after\nfig, axes = plt.subplots(1, 2, figsize=(10,5))\n\naxes[0].imshow(original_image)\naxes[0].set_title(\"Original\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(image)\naxes[1].set_title(\"Adjusted\")\naxes[1].axis(\"off\")\n\nplt.show()"
  },
  {
    "objectID": "example1.html#perform-an-analysis",
    "href": "example1.html#perform-an-analysis",
    "title": "Example 1",
    "section": "Perform an analysis",
    "text": "Perform an analysis\n\ndetect and set the processor to use\n\nif torch.cuda.is_available():\n    cc_major, cc_minor = torch.cuda.get_device_capability(0)\n    if cc_major &lt; 7:  # unsupported by current PyTorch build\n        print(f\"GPU {torch.cuda.get_device_name(0)} (compute capability {cc_major}.{cc_minor})\\n\"\n              f\"is not supported by this PyTorch build. Falling back to CPU.\")\n        device = torch.device(\"cpu\")\n        print(\"Using CPU\")\n        print(\"CPU info:\", platform.processor())\n        print(\"Machine type:\", platform.machine())\n        print(\"Platform:\", platform.platform())\n    else:\n        device = torch.device(\"cuda:0\")\n        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU\")\n    print(\"CPU info:\", platform.processor())\n    print(\"Machine type:\", platform.machine())\n    print(\"Platform:\", platform.platform())\n\nUsing CPU\nCPU info: Intel64 Family 6 Model 151 Stepping 2, GenuineIntel\nMachine type: AMD64\nPlatform: Windows-10-10.0.26200-SP0\n\n\n\n\nload the model\nThis model is obtained from the Meta.\n\n#what sam model to use (see their github for diff options, but b is the smallest one = less crashing)\nsam_checkpoint = \"sam_vit_b_01ec64.pth\"\nmodel_type = \"vit_b\"\n\n#added print statements to check due to previous crashing issues\ntry: \n    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n    print(\"YUP all loaded\")\nexcept Exception as e:\n    print(\"NOPE not loaded\")\n\nYUP all loaded\n\n\n\n\nSet up to run the SAM model\n\nsam.to(device=device)\n\n#Below are parameters that seem to be good for chaing min size allowed, how detailed the model looks for masks, etc \n\n\n#There are several tunable parameters in automatic mask generation that control \n# how densely points are sampled and what the thresholds are for removing low \n# quality or duplicate masks. Additionally, generation can be automatically \n# run on crops of the image to get improved performance on smaller objects, \n# and post-processing can remove stray pixels and holes. \n# Here is an example configuration that samples more masks:\n#https://github.com/facebookresearch/segment-anything/blob/9e1eb9fdbc4bca4cd0d948b8ae7fe505d9f4ebc7/segment_anything/automatic_mask_generator.py#L35    \n\n#Rerun the following with a few settings, ex. 0.86 & 0.9 for iou_thresh\n# and 0.92 and 0.96 for score_thresh\n\nmask_generator_ = SamAutomaticMaskGenerator(\n     model=sam,\n    points_per_side=32,\n    pred_iou_thresh=0.9,\n    stability_score_thresh=0.96,\n    crop_n_layers=1,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=100,    # Requires open-cv to run post-processing\n)\n\nApply the defined mask generator to process the image.\nThis a very expensive (time consuming) process.\n\nstart = time.perf_counter()\n\nmasks = mask_generator_.generate(image)\n\nend = time.perf_counter()\nelapsed = end - start\n\nprint(\"The number of masks:\", len(masks))\nprint(f\"Mask generation took {elapsed:.2f} seconds\")\n\nThe number of masks: 18\nMask generation took 204.19 seconds\n\n\n\n\nPresent image with cells identified\nUsing the masks created above, plot the segments/cells.\n\ndef show_anns(anns):\n    # Function to loop over segments identified and display them on plot\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n    polygons = []\n    color = []\n    for ann in sorted_anns:\n        m = ann['segmentation']\n        img = np.ones((m.shape[0], m.shape[1], 3))\n        color_mask = np.random.random((1, 3)).tolist()[0]\n        for i in range(3):\n            img[:,:,i] = color_mask[i]\n        ax.imshow(np.dstack((img, m*0.35)))\n\n#output the image with colors for the masks\n\nplt.figure(figsize=(5,5))\nplt.imshow(image)\nshow_anns(masks)\nplt.axis('off')\nplt.show() \n\n\n\n\n\n\n\n\n\n\nCompute area\n\n#analysis\ntotal_areas = [] #initialize the array\nfor mask in masks:\n    #check if the mask is a bubble\n    if mask[\"area\"] &gt; 350:\n        # add non-bubble masks to the array\n        total_areas.append(mask[\"area\"])\n\n#remove the background element\ndel total_areas[0]\n\n#convert to a numpy array\ntotal_areas = np.array(total_areas)\n#sort the array from least to greatest (just easier to look at)\ntotal_areas.sort()\n\nfor area in total_areas:\n    print(area)\n\n511\n588\n1023\n2224\n3194\n3438\n3772\n4221\n4988688\n\n\n\n\nCompute\n\n#find images average area in pixels\nmean_area = np.mean(total_areas)\n\nprint(f\"The mean area in pixels is {mean_area:.2f}\")\n\nThe mean area in pixels is 556406.56\n\n\n\n\nPerform cell classificition\nWe’ll identify cells based on cell size (area)\n\n#Attempt to find overall cell type based on mean of cell area \nif mean_area &gt;= 10000:\n    print(\"buccal cells\")\nelif mean_area &lt;2500:\n    print(\"touch cells\")\nelif mean_area &gt;=2500 and mean_area &lt; 10000:\n    print(\"saliva cells\")\n\n# how many of each type\nbuccalCount = 0\ntouchCount = 0\nsalivaCount = 0\n\nfor area in total_areas:\n    if area &gt;= 10000:\n        buccalCount +=1\n    elif area &lt;2500:\n        touchCount +=1\n    elif area &gt;=2500 and area &lt; 10000:\n        salivaCount +=1\n\nprint(f\"\"\"\nThere are:\n{buccalCount} buccal cells,\n{touchCount} touch cells, and\n{salivaCount} saliva cells\n\"\"\")\n\nbuccal cells\n\nThere are:\n1 buccal cells,\n4 touch cells, and\n4 saliva cells"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "First draft of a working website documenting our work on this project."
  },
  {
    "objectID": "index.html#intro-presentation",
    "href": "index.html#intro-presentation",
    "title": "Welcome",
    "section": "Intro presentation",
    "text": "Intro presentation\nBelow is the intro presentation shared by Alina. Here a direct link to the google slides source."
  },
  {
    "objectID": "image-gallery-guide.html",
    "href": "image-gallery-guide.html",
    "title": "Image Gallery and Upload Guide",
    "section": "",
    "text": "This guide explains how to use the forensic cell image gallery and upload functionality in the SSG High School Forensics Application. The system allows users to view sample images, upload their own cell images, and process them using AI-powered analysis tools."
  },
  {
    "objectID": "image-gallery-guide.html#overview",
    "href": "image-gallery-guide.html#overview",
    "title": "Image Gallery and Upload Guide",
    "section": "",
    "text": "This guide explains how to use the forensic cell image gallery and upload functionality in the SSG High School Forensics Application. The system allows users to view sample images, upload their own cell images, and process them using AI-powered analysis tools."
  },
  {
    "objectID": "image-gallery-guide.html#image-gallery",
    "href": "image-gallery-guide.html#image-gallery",
    "title": "Image Gallery and Upload Guide",
    "section": "Image Gallery",
    "text": "Image Gallery\nThe image gallery is organized into three main sections, each serving a specific purpose in the forensic analysis workflow.\n\nSample Images\nThe Sample Images section contains pre-loaded reference images of different cell types:\n\nBuccal Cells: Epithelial cells collected from the inside of the cheek, commonly used in forensic DNA analysis\nEpidermal Cells: Skin cells that can be found in fingerprints and touch DNA samples\nSaliva Cells: A mixture of epithelial cells found in biological evidence\n\nThese images are provided for educational purposes and can be used to:\n\nLearn to identify different cell types\nPractice with the image processing tools\nCompare against uploaded images\n\n\nUsing Sample Images\n\nNavigate to the gallery page\nBrowse images organized by cell type\nHover over any image to reveal action buttons:\n\nView: Opens the full-size image in a new tab\nProcess: Sends the image to the AI analysis pipeline\n\n\n\n\n\nUploaded Images\nThis section displays all images that have been uploaded by users through the upload tool.\n\nHow to Upload Images\n\nNavigate to the Upload page\nClick the upload area or drag and drop an image file\nSupported formats: JPG, JPEG, PNG\nOnce uploaded, the image will appear in the Uploaded Images section of the gallery\n\n\n\nProcessing Uploaded Images\nAfter uploading an image:\n\nGo to the gallery page\nFind your image in the Uploaded Images section\nHover over the image to reveal action buttons\nClick Process to analyze the image\nWait for the processing to complete (indicated by status messages)\nView the processed result in the Processed Images section\n\n\n\n\nProcessed Images\nThis section contains all images that have been analyzed by the AI segmentation and identification tools.\nProcessed images include:\n\nSegmented cell boundaries\nCell type identification\nAnalysis metadata\n\n\nViewing Processed Results\n\nClick on any processed image to view it full-size\nProcessed images are named with a processed_ prefix\nResults are stored persistently and can be reviewed at any time"
  },
  {
    "objectID": "image-gallery-guide.html#technical-implementation",
    "href": "image-gallery-guide.html#technical-implementation",
    "title": "Image Gallery and Upload Guide",
    "section": "Technical Implementation",
    "text": "Technical Implementation\n\n\n\n\n\n\nNote\n\n\n\nThis section contains technical details about the system architecture and implementation. Feel free to skip this if you’re primarily interested in using the application.\n\n\n\nArchitecture\nThe gallery system uses a dynamic, API-driven architecture:\nFrontend (Quarto/HTML/JS) → NGINX → FastAPI Backend → File System\n\n\nAPI Endpoints\nThe backend provides three main endpoints for the gallery:\n\nGET /api/gallery/sample-images: Returns organized list of sample images by cell type\nGET /api/gallery/uploaded-images: Returns list of user-uploaded images\nGET /api/gallery/processed-images: Returns list of processed images\n\n\n\nImage Storage\nImages are organized in the following directory structure:\nimages/\n├── sample-gallery-images/\n│   ├── buccal_cells/\n│   ├── epidermal_cells/\n│   └── saliva_cells/\n├── uploaded-images/\n└── processed-images/\n\n\nProcessing Workflow\n\nUser selects an image to process\nFrontend fetches the image as a blob\nImage is sent to /api/process-img endpoint via POST request\nBackend processes the image using the SAM2 model (or current processing logic)\nProcessed image is saved to processed-images/ directory\nFrontend refreshes the Processed Images gallery\nUser is notified of successful processing"
  },
  {
    "objectID": "image-gallery-guide.html#best-practices",
    "href": "image-gallery-guide.html#best-practices",
    "title": "Image Gallery and Upload Guide",
    "section": "Best Practices",
    "text": "Best Practices\n\nFor Students\n\nStart by exploring sample images to learn different cell types\nUpload clear, well-focused microscope images for best results\nProcess images one at a time to avoid overwhelming the system\nCompare processed results against sample images to verify accuracy\n\n\n\nFor Educators\n\nUse sample images for demonstrations and teaching\nGuide students to upload their own collected samples\nReview processed images with students to discuss cell identification\nLeverage the gallery for comparative analysis exercises"
  },
  {
    "objectID": "image-gallery-guide.html#troubleshooting",
    "href": "image-gallery-guide.html#troubleshooting",
    "title": "Image Gallery and Upload Guide",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nImages Not Loading\n\nCheck that Docker containers are running\nVerify volume mounts in docker-compose.yml\nCheck browser console for API errors\n\n\n\nUpload Fails\n\nEnsure file is a supported format (JPG, PNG)\nCheck file size is reasonable\nVerify backend is accessible at /api\n\n\n\nProcessing Fails\n\nCheck that the image file exists and is accessible\nVerify backend processing pipeline is configured\nReview backend logs for error messages"
  },
  {
    "objectID": "image-gallery-guide.html#future-enhancements",
    "href": "image-gallery-guide.html#future-enhancements",
    "title": "Image Gallery and Upload Guide",
    "section": "Future Enhancements",
    "text": "Future Enhancements\nPotential improvements to the gallery system:\n\nImage metadata display (size, dimensions, upload date)\nFiltering and search capabilities\nBatch processing support\nExport functionality for processed images\nComparison view for side-by-side analysis\nUser annotations and notes"
  },
  {
    "objectID": "development.html",
    "href": "development.html",
    "title": "Developer Documentation",
    "section": "",
    "text": "This guide is intended for developers and contributors interested in understanding the architecture and implementation details of the SSG High School Forensics Application. It provides insights into the project structure, technology stack, and key components.\n\n\n\n\n\n\nWarning\n\n\n\nIf you are looking for documentation on how to create lessons for the application, please see the Creating New Lessons guide instead."
  },
  {
    "objectID": "development.html#who-this-page-is-for",
    "href": "development.html#who-this-page-is-for",
    "title": "Developer Documentation",
    "section": "Who This Page Is For",
    "text": "Who This Page Is For\nThis section of the documentation is intended for developers or those who are hoping to look under the hood of this project. If you know little about coding or are trying to figure out how to add your own lessons to the application, then this section of documentation is NOT for you. If you are looking for documentation about how to make your own lesson, please see the Creating New Lessons guide."
  },
  {
    "objectID": "development.html#location-of-files",
    "href": "development.html#location-of-files",
    "title": "Developer Documentation",
    "section": "Location of Files",
    "text": "Location of Files\nThese file locations will be relative to the root directory (ssg-hs-forensics-app/).\nforensics-ui/ - contains the files used to generate the graphics (the actual website itself). Within this directory: * _site/ - contains the rendered pages of the website. * .quarto - contains the intermediate files generated by Quarto before the files in ’_site/’ are created. (DOUBLE CHECK THAT THIS IS CORRECT) * cell-gallery/ - contains the Quarto/QMD and CSS files needed to generate the interactive cell gallery; whenever the ‘process’ button is pressed, it sends the specific image to the backend for processing and retrieves the processed image. * lesson-1/ - contains the QMD and CSS files needed to generate the lesson pages of the first lesson. * upload/ - contains the QMD and CSS files responsible for upload functionality; sends uploaded images to the backend for processing and retrieves the results. * _quarto.yml - file responsible for coordinating the other files for the front-end, and provides the website bar necessary to navigate the different pages of the website.\nforensics-backend/ - contains the files that manage the whole system and process uploaded images. Within this directory: * app/ - contains the following files: - Dockerfile - manages setting up the Docker compartments and running the server. - main.py - contains the server functions, which allow images to be processed and retrieved. - requirements.txt - contains the names of Python libraries required for this project; ‘Dockerfile’ uses this file to know what libraries to install when setting up the Docker compartments and webserver. - run_sam2.py - responsible for processing uploaded images; used by ‘main.py’. * nginx/ - contains the following file: - default.conf - configurations for the Docker compartments; manages pathways between systems, including connecting containers to the outside world and rerouting calls to their intended compartments. * docker-compose.yml - responsible for Docker container creation, and leaves ‘windows’ open so files outside certain compartments can be used.\nimages/ - contains the images used in the website. Within this directory: * processed-images/ - contains the processed images (images that have been run through the SAM model). * sample-gallery-images/ - contains the sample images that are pre-loaded into the gallery. * uploaded-images/ - contains the images users have uploaded.\nreports/ - contains the QMD files that render into the documentation for this project. Within this directory: (TODO: ADD TO) * development.qmd - the file that renders this very file; documentation indended for developers or those who are interested in the behind-the-scenes information."
  },
  {
    "objectID": "development.html#overview-of-coding-languages-used",
    "href": "development.html#overview-of-coding-languages-used",
    "title": "Developer Documentation",
    "section": "Overview of Coding Languages Used",
    "text": "Overview of Coding Languages Used\nThis project primarily uses Python, JavaScript, CSS, and HTML (through Quarto). The actual processing of files relies on Python code. JavaScript is used within the QMD (Quarto) files in order to add interactivity to the generated HTML pages, including APIs that allow images to be sent to the backend server for processing and to request images from the server. Quarto is used to generate the HTML files that make up the website."
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "Welcome",
    "section": "Documentation",
    "text": "Documentation\n\nImage Gallery and Upload Guide - Learn how to upload images and use the gallery features\nDeveloper Documentation - Technical documentation for developers working on the project\nCreating New Lessons - Guide for developers to create new lesson content\nGITHUB repository with source and other documents - our project repository"
  },
  {
    "objectID": "lesson-creation-guide.html",
    "href": "lesson-creation-guide.html",
    "title": "Creating New Lessons",
    "section": "",
    "text": "WarningTechnical Users Only\n\n\n\nThis guide is intended for developers and technical users who are comfortable with HTML, Quarto, and web development. If you’re looking for user guides on how to use the application, please see the Image Gallery and Upload Guide instead."
  },
  {
    "objectID": "lesson-creation-guide.html#overview",
    "href": "lesson-creation-guide.html#overview",
    "title": "Creating New Lessons",
    "section": "Overview",
    "text": "Overview\nThis guide explains how to create new forensic science lessons for the SSG High School Forensics Application. Lessons are built using Quarto (QMD files) and can include text, images, videos, interactive quizzes, and custom styling."
  },
  {
    "objectID": "lesson-creation-guide.html#lesson-structure",
    "href": "lesson-creation-guide.html#lesson-structure",
    "title": "Creating New Lessons",
    "section": "Lesson Structure",
    "text": "Lesson Structure\n\nDirectory Organization\nEach lesson is contained in its own directory within forensics-ui/. The structure follows this pattern:\nforensics-ui/\n├── lesson-1/           # Example lesson\n│   ├── index.qmd       # Lesson landing page\n│   ├── 1-objectives.qmd\n│   ├── 2-saliva.qmd\n│   ├── 3-skin.qmd\n│   ├── 4-blood.qmd\n│   ├── 5-buccal.qmd\n│   ├── 6-epithelial.qmd\n│   ├── 7-quiz.qmd\n│   ├── 8-analysis-introduction.qmd\n│   ├── styles.css      # Lesson-specific styles\n│   └── assets/         # Images, videos, etc.\n└── lesson-2/           # Your new lesson\n    ├── index.qmd\n    ├── 1-topic.qmd\n    ├── 2-topic.qmd\n    └── ..."
  },
  {
    "objectID": "lesson-creation-guide.html#creating-a-new-lesson",
    "href": "lesson-creation-guide.html#creating-a-new-lesson",
    "title": "Creating New Lessons",
    "section": "Creating a New Lesson",
    "text": "Creating a New Lesson\n\nStep 1: Create Lesson Directory\n\nNavigate to forensics-ui/\nCreate a new directory with a descriptive name (e.g., lesson-2, dna-analysis, etc.)\n\ncd forensics-ui\nmkdir lesson-2\ncd lesson-2\n\n\nStep 2: Create the Index Page\nCreate an index.qmd file as the lesson landing page:\n---\ntitle: \"Lesson 2: DNA Analysis\"\n---\n\n## Introduction\n\nBrief overview of what this lesson covers.\n\n## Topics Covered\n\n- [Topic 1](1-topic1.qmd)\n- [Topic 2](2-topic2.qmd)\n- [Quiz](3-quiz.qmd)\n\n\nStep 3: Create Lesson Pages\nEach lesson page should be a separate QMD file. Number them sequentially for organization.\nExample: 1-introduction.qmd\n---\ntitle: \"Introduction to DNA\"\n---\n\n## What is DNA?\n\nYour content here...\n\n### Key Concepts\n\n- Concept 1\n- Concept 2\n\n## Next Steps\n\n[Continue to next section →](2-structure.qmd)\n\n\nStep 4: Add Custom Styling (Optional)\nCreate a styles.css file for lesson-specific styling:\n/* Lesson 2 Custom Styles */\n.lesson-header {\n  background-color: #4a90e2;\n  padding: 2rem;\n  color: white;\n}\n\n.key-term {\n  font-weight: bold;\n  color: #2c3e50;\n}\nReference it in your QMD files:\n---\ntitle: \"Page Title\"\nformat:\n  html:\n    css: styles.css\n---\n\n\nStep 5: Add to Site Navigation\nEdit forensics-ui/_quarto.yml to add your lesson to the site navigation:\nwebsite:\n  navbar:\n    left:\n      - text: Home\n        href: index.qmd\n      - text: Lesson 1\n        href: lesson-1/index.qmd\n      - text: Lesson 2        # Add this\n        href: lesson-2/index.qmd\n      - text: Gallery\n        href: cell-gallery/gallery.qmd\n\n\nStep 6: Render and Test\ncd forensics-ui\nquarto render\nOpen the generated site in _site/ to test your lesson."
  },
  {
    "objectID": "lesson-creation-guide.html#lesson-components",
    "href": "lesson-creation-guide.html#lesson-components",
    "title": "Creating New Lessons",
    "section": "Lesson Components",
    "text": "Lesson Components\n\nText and Markdown\nUse standard Markdown syntax:\n# Heading 1\n## Heading 2\n\n**Bold text**\n*Italic text*\n\n- Bullet list\n1. Numbered list\n\n[Link text](url)\n\n\nImages\nPlace images in an assets/ folder within your lesson directory:\n![Image description](assets/dna-structure.png)\n\n\nVideos\nEmbed videos using HTML:\n```{=html}\n&lt;video width=\"100%\" controls&gt;\n  &lt;source src=\"assets/video.mp4\" type=\"video/mp4\"&gt;\n&lt;/video&gt;\n\nOr embed from external sources:\n\n```markdown\n```{=html}\n&lt;iframe width=\"560\" height=\"315\" \n  src=\"https://www.youtube.com/embed/VIDEO_ID\" \n  frameborder=\"0\" allowfullscreen&gt;\n&lt;/iframe&gt;\n\n### Callouts and Alerts\n\nUse Quarto's built-in callout blocks:\n\n```markdown\n:::{.callout-note}\nThis is an important note for students.\n:::\n\n:::{.callout-tip}\nHere's a helpful tip!\n:::\n\n:::{.callout-warning}\nPay attention to this warning.\n:::\n\n\nInteractive Quizzes\nCreate quiz pages with HTML forms:\n&lt;form id=\"quiz\"&gt;\n  &lt;div class=\"question\"&gt;\n    &lt;p&gt;1. What is DNA?&lt;/p&gt;\n    &lt;label&gt;&lt;input type=\"radio\" name=\"q1\" value=\"a\"&gt; Answer A&lt;/label&gt;\n    &lt;label&gt;&lt;input type=\"radio\" name=\"q1\" value=\"b\"&gt; Answer B&lt;/label&gt;\n    &lt;label&gt;&lt;input type=\"radio\" name=\"q1\" value=\"c\"&gt; Answer C&lt;/label&gt;\n  &lt;/div&gt;\n  \n  &lt;button type=\"submit\"&gt;Submit Quiz&lt;/button&gt;\n&lt;/form&gt;\n\n&lt;script&gt;\ndocument.getElementById('quiz').addEventListener('submit', function(e) {\n  e.preventDefault();\n  // Quiz logic here\n  alert('Quiz submitted!');\n});\n&lt;/script&gt;\n\n\nCode Blocks\nDisplay code with syntax highlighting:\n```python\ndef analyze_dna(sequence):\n    return sequence.upper()\n```"
  },
  {
    "objectID": "lesson-creation-guide.html#best-practices",
    "href": "lesson-creation-guide.html#best-practices",
    "title": "Creating New Lessons",
    "section": "Best Practices",
    "text": "Best Practices\n\nContent Organization\n\nBreak content into manageable chunks: Each page should cover one main topic\nUse descriptive filenames: 3-dna-extraction.qmd is better than page3.qmd\nNumber pages sequentially: Makes navigation and ordering clear\nInclude navigation links: Add “Previous” and “Next” links at the bottom of each page\n\n\n\nStyling\n\nKeep styles consistent: Use existing lesson styles as a template\nUse semantic HTML: Use &lt;article&gt;, &lt;section&gt;, etc. appropriately\nMake it responsive: Test on different screen sizes\nFollow accessibility guidelines: Use alt text, proper heading hierarchy, etc.\n\n\n\nMedia Files\n\nOptimize images: Compress images before adding them\nUse web-friendly formats: JPG for photos, PNG for graphics, WebP when possible\nProvide alt text: Always include descriptive alt text for images\nHost large videos externally: Use YouTube or Vimeo for large video files\n\n\n\nNavigation\n\nProvide clear navigation: Table of contents, breadcrumbs, next/previous links\nLink back to lesson index: Always provide a way to return to the lesson overview\nUse descriptive link text: “Learn about DNA structure” instead of “Click here”"
  },
  {
    "objectID": "lesson-creation-guide.html#example-complete-lesson-page",
    "href": "lesson-creation-guide.html#example-complete-lesson-page",
    "title": "Creating New Lessons",
    "section": "Example: Complete Lesson Page",
    "text": "Example: Complete Lesson Page\n---\ntitle: \"DNA Structure\"\nformat:\n  html:\n    css: styles.css\n---\n\n[← Back to Lesson Index](index.qmd)\n\n## Introduction\n\nDNA (Deoxyribonucleic Acid) is the hereditary material in humans and most other organisms.\n\n:::{.callout-note}\nDNA is found in nearly every cell of the human body!\n:::\n\n## The Double Helix\n\n![DNA Double Helix](assets/dna-helix.png){width=50%}\n\nThe structure of DNA was discovered by Watson and Crick in 1953.\n\n### Key Components\n\n1. **Sugar-phosphate backbone**\n2. **Nitrogenous bases**\n   - Adenine (A)\n   - Thymine (T)\n   - Guanine (G)\n   - Cytosine (C)\n\n## Interactive Activity\n\nTry identifying the bases in this DNA sequence:\n\n&lt;div class=\"activity\"&gt;\n  &lt;p&gt;Sequence: ATCG TACG GCTA&lt;/p&gt;\n  &lt;input type=\"text\" placeholder=\"Your answer\"&gt;\n  &lt;button&gt;Check Answer&lt;/button&gt;\n&lt;/div&gt;\n\n## Quiz Yourself\n\n:::{.callout-tip}\nTest your knowledge with the [DNA Structure Quiz →](quiz.qmd)\n:::\n\n---\n\n[← Previous: Introduction](1-introduction.qmd) | [Next: DNA Extraction →](3-extraction.qmd)"
  },
  {
    "objectID": "lesson-creation-guide.html#testing-your-lesson",
    "href": "lesson-creation-guide.html#testing-your-lesson",
    "title": "Creating New Lessons",
    "section": "Testing Your Lesson",
    "text": "Testing Your Lesson\n\nLocal Testing\n\nRender the lesson:\ncd forensics-ui\nquarto render lesson-2/\nCheck the output in forensics-ui/_site/lesson-2/\nOpen in a browser to test:\n# Start a local server\npython -m http.server 8080 --directory _site\nNavigate to http://localhost:8080/lesson-2/\n\n\n\nChecklist\n\nAll links work correctly\nImages load properly\nVideos play correctly\nQuizzes function as expected\nNavigation is clear and intuitive\nContent is accessible (alt text, headings, etc.)\nStyling is consistent with other lessons\nMobile responsive design works"
  },
  {
    "objectID": "lesson-creation-guide.html#deploying-your-lesson",
    "href": "lesson-creation-guide.html#deploying-your-lesson",
    "title": "Creating New Lessons",
    "section": "Deploying Your Lesson",
    "text": "Deploying Your Lesson\nOnce your lesson is complete and tested:\n\nEnsure all files are committed to version control\nRender the entire site:\ncd forensics-ui\nquarto render\nThe Docker setup will serve the new lesson automatically"
  },
  {
    "objectID": "lesson-creation-guide.html#troubleshooting",
    "href": "lesson-creation-guide.html#troubleshooting",
    "title": "Creating New Lessons",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nLesson Not Appearing in Navigation\n\nCheck that you added it to _quarto.yml\nVerify the file path is correct\nRe-render the entire site\n\n\n\nStyles Not Applying\n\nEnsure css: styles.css is in the YAML front matter\nCheck that the CSS file is in the correct directory\nClear browser cache and refresh\n\n\n\nImages Not Loading\n\nVerify image paths are correct (relative to the QMD file)\nCheck that images are in the assets/ folder\nEnsure image file names match exactly (case-sensitive)\n\n\n\nVideos Not Playing\n\nTest video format compatibility (MP4 is most widely supported)\nCheck video file size (large files may need external hosting)\nVerify video codec is web-compatible"
  },
  {
    "objectID": "lesson-creation-guide.html#additional-resources",
    "href": "lesson-creation-guide.html#additional-resources",
    "title": "Creating New Lessons",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nQuarto Documentation\nQuarto Website Guide\nMarkdown Basics\nHTML in Quarto"
  },
  {
    "objectID": "lesson-creation-guide.html#getting-help",
    "href": "lesson-creation-guide.html#getting-help",
    "title": "Creating New Lessons",
    "section": "Getting Help",
    "text": "Getting Help\nIf you encounter issues or have questions:\n\nCheck the existing lessons for examples\nReview Quarto documentation\nConsult the development team\nCheck browser console for errors"
  },
  {
    "objectID": "index.html#example-quarto-runs",
    "href": "index.html#example-quarto-runs",
    "title": "Welcome",
    "section": "Example Quarto runs",
    "text": "Example Quarto runs\n\nUsing Quarto and SAM to process an image - This is the file that started it all! Alina brought a Jupyter notebook to SSG. Our first step was to convert it to a Quarto QMD. This is the Quarto QMD. Use this file as a template when wanting to run the SAM model and create quick-hit reports of cell images. It provides examples of how to load the image, process the image with SAM, and present the results in a clean, quarto HTML report."
  },
  {
    "objectID": "development.html#overview",
    "href": "development.html#overview",
    "title": "Developer Documentation",
    "section": "",
    "text": "This guide is intended for developers and contributors interested in understanding the architecture and implementation details of the SSG High School Forensics Application. It provides insights into the project structure, technology stack, and key components.\n\n\n\n\n\n\nWarning\n\n\n\nIf you are looking for documentation on how to create lessons for the application, please see the Creating New Lessons guide instead."
  },
  {
    "objectID": "development.html#important-notes",
    "href": "development.html#important-notes",
    "title": "Developer Documentation",
    "section": "Important Notes",
    "text": "Important Notes\n\n\n\n\n\n\nNote\n\n\n\nImage Folder Optimization: The sample gallery images directory should ideally contain pre-processed images so that (1) users don’t need to wait for initial processing and (2) user-processed images remain separate from sample images.\n\n\nFor more detailed implementation information, developers should refer to code comments and docstrings located directly in the relevant source files."
  },
  {
    "objectID": "development.html#project-structure",
    "href": "development.html#project-structure",
    "title": "Developer Documentation",
    "section": "Project Structure",
    "text": "Project Structure\nAll file locations are relative to the root directory: ssg-hs-forensics-app/\n\nFrontend: forensics-ui/\nThis directory contains all files used to generate the application website and user interface.\n\n\n\n\n\n\n\nDirectory/File\nPurpose\n\n\n\n\n_site/\nRendered HTML pages of the website (output from Quarto build)\n\n\n.quarto/\nIntermediate files generated by Quarto during the build process\n\n\n_quarto.yml\nMaster configuration file that coordinates all frontend components and generates the navigation bar\n\n\ncell-gallery/\nInteractive cell image gallery; sends user selections to backend for processing\n\n\nlesson-1/, lesson-3/\nLesson content files (QMD format) with associated stylesheets\n\n\nupload/\nUpload functionality for user-submitted microscope images\n\n\nstyles.css\nGlobal styling for the application\n\n\n\n\n\nBackend: forensics-backend/\nThis directory manages image processing and server functionality.\nforensics-backend/\n├── app/\n│   ├── Dockerfile           # Docker image configuration\n│   ├── main.py              # FastAPI server with image processing endpoints\n│   ├── requirements.txt      # Python dependencies\n│   ├── run_sam2.py          # SAM2 model execution\n│   └── runsam.py            # Legacy SAM model execution\n├── nginx/\n│   └── default.conf         # NGINX reverse proxy configuration\n└── docker-compose.yml       # Docker multi-container orchestration\nKey Components:\n\nmain.py: Contains FastAPI endpoints for image processing and retrieval\nrun_sam2.py: Handles image segmentation and analysis using the SAM2 model\ndocker-compose.yml: Orchestrates FastAPI and NGINX containers\ndefault.conf: Routes requests between frontend and backend services\n\n\n\nImage Storage: images/\nImages are organized by processing state:\nimages/\n├── sample-gallery-images/   # Pre-loaded reference images (organized by cell type)\n│   ├── buccal_cells/\n│   ├── epidermal_cells/\n│   └── saliva_cells/\n├── uploaded-images/         # User-submitted images\n└── processed-images/        # Results from SAM model processing\n\n\nDocumentation: reports/\nContains Quarto markdown files that generate developer and user documentation.\n\n\n\n\n\n\n\nFile\nPurpose\n\n\n\n\ndevelopment.qmd\nThis file; developer-focused documentation\n\n\nlesson-creation-guide.qmd\nGuide for educators creating new lessons\n\n\nimage-gallery-guide.qmd\nUser guide for the image gallery and upload features"
  },
  {
    "objectID": "development.html#technology-stack",
    "href": "development.html#technology-stack",
    "title": "Developer Documentation",
    "section": "Technology Stack",
    "text": "Technology Stack\nThis project utilizes a modern, modular architecture with clear separation between frontend and backend components.\n\nFrontend Technologies\n\nQuarto: Generates static HTML pages from markdown (QMD) files\nHTML/CSS: Structure and styling of user interface\nJavaScript: Client-side interactivity and API communication\n\n\n\nBackend Technologies\n\nPython: Core processing logic and server implementation\nFastAPI: RESTful API framework for image processing endpoints\nSAM2 Model: AI-powered image segmentation for cell identification\nDocker & NGINX: Container orchestration and reverse proxy configuration\n\n\n\nProcessing Pipeline\nThe image processing workflow follows this architecture:\nUser Browser\n    ↓\nFrontend (JavaScript/HTML)\n    ↓\nNGINX Reverse Proxy\n    ↓\nFastAPI Backend (main.py)\n    ↓\nSAM2 Model (run_sam2.py)\n    ↓\nFile System (processed-images/)\n    ↓\nResults returned to Frontend"
  },
  {
    "objectID": "development.html#key-architectural-patterns",
    "href": "development.html#key-architectural-patterns",
    "title": "Developer Documentation",
    "section": "Key Architectural Patterns",
    "text": "Key Architectural Patterns\n\nFrontend-Backend Communication\nThe frontend communicates with the backend through HTTP requests:\n\nUpload images: POST to /api/process-img\nRetrieve images: GET from /api/gallery/ endpoints\nStatus queries: GET requests for processing status\n\n\n\nImage Processing Workflow\n\nUser selects or uploads an image\nFrontend encodes image as blob and sends to backend\nBackend receives image and queues for SAM2 processing\nSAM2 model generates cell segmentation masks\nProcessed image is saved to disk\nResponse sent back to frontend\nGallery automatically refreshes\n\n\n\nContainer Architecture\nThe Docker Compose setup creates isolated, interconnected containers:\n\nFastAPI Container: Runs the Python application server\nNGINX Container: Acts as reverse proxy and load balancer\nVolume Mounts: Allow containers to access shared image directories"
  },
  {
    "objectID": "development.html#development-best-practices",
    "href": "development.html#development-best-practices",
    "title": "Developer Documentation",
    "section": "Development Best Practices",
    "text": "Development Best Practices\n\nCode Organization\nWhen adding new features, follow these patterns:\n\nBackend endpoints: Define in forensics-backend/app/main.py\nFrontend logic: Add to lesson/gallery QMD files as JavaScript code blocks\nShared utilities: Consider adding to dedicated modules rather than inline code\nConfiguration: Use environment variables and config files rather than hardcoding values\n\n\n\nDebugging and Logging\n\nBackend logs are available through Docker: docker compose logs -f app\nFrontend errors appear in browser console (F12)\nAdd print statements or logging in Python for backend diagnostics\n\n\n\nTesting Considerations\n\nTest image processing with various image formats and sizes\nVerify NGINX routing between containers\nTest frontend-backend communication with different network conditions"
  },
  {
    "objectID": "development.html#building-and-deployment",
    "href": "development.html#building-and-deployment",
    "title": "Developer Documentation",
    "section": "Building and Deployment",
    "text": "Building and Deployment\nTo set up the development environment:\n# Install dependencies\nmake install\n\n# Start the application\nmake run\n\n# Build documentation\nmake docs\nRefer to the Makefile and README.md for complete build instructions and advanced deployment options."
  }
]